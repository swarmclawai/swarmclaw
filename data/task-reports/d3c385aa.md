# Task d3c385aa: Rearchitect memories system with file references, images, and linking

- Status: completed
- Agent: fcd98bac
- Session: a680c367

## Description
Rearchitect SwarmClaw memories system with the following:

## Core Features
1. **File/folder references**: Memories can store file paths with optional context snippets (e.g., "I wrote this function in this file" but NOT full file contents). Include timestamp of when the reference was created.

2. **Image support**: Memories can contain images. Images are stored externally (not in DB), compressed but still usable by multimodal models. Store compressed image file on disk, reference path in memory record. Compression should balance quality vs size.

3. **Memory linking**: Memories can reference other memories. Implement bidirectional or parent-child linking. Support traversal with configurable depth limit.

4. **Configurable limits**: 
 - Default max depth: 3
 - Default max memories per lookup: 20
 - Both configurable via settings

5. **Updated memory_tool interface**: Consider both:
 - Pass `linkedMemoryIds` at store time
 - `link_memory` action to link existing memories

## Implementation Requirements

### CLI Updates
- Update memory_tool to support new features (linking, file refs, images)
- Add commands for:
 - Storing memories with file references
 - Storing memories with images
 - Linking memories
 - Configuring depth/limit settings
 
### API Updates
- Update `/api/memories` endpoints to handle:
 - File reference storage
 - Image upload and compression (save to disk, return path)
 - Memory linking (schema + CRUD)
 - Depth-limited retrieval with linked memories
- Add settings endpoints for configurable limits

### Database/Schema
- Add columns/tables for:
 - Memory links (junction table or adjacency list)
 - File references (path, context, timestamp)
 - Image path references
- Migration: existing memories should work unchanged

### Testing
- Test file reference creation and retrieval
- Test image upload, compression, and retrieval
- Test memory linking at various depths
- Test configurable limit enforcement

## Migration Strategy
- Existing memories remain valid without changes
- New fields should be nullable or have defaults
- Migration script to add new columns/tables without data loss

## Result Summary
Good, `crypto` is already imported. But wait — in the `store` action I'm generating `entryId` with `crypto.randomBytes` but the actual ID is generated inside `memDb.add()`. The `entryId` I create is just for the image path, and the actual memory gets a different ID. Let me fix this — the image should use the actual memory ID, or I should pass the image handling into the add method. Actually, the simpler approach: just use the entryId for the image filename (it doesn't need to match the memory ID, it just needs to be unique), or better yet, let me adjust so the image path uses a standalone UUID since the memory ID is generated inside `add()`.

Actually, looking at this more carefully, the current approach is fine — the image filename uses a separate random ID which is unique enough. The memory's `imagePath` field will store the correct path regardless. Let me leave it as-is.

Now update the API routes.

## Changed Files
- Not provided

## Commands Run
- Not provided

## Verification
- Not provided
